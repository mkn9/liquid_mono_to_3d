# Complete imports and matplotlib setup
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend first
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
import cv2
import os
import sys

# Set up matplotlib for static plots (no JavaScript dependencies)
%matplotlib inline

print("All imports loaded successfully")
print("Matplotlib backend:", matplotlib.get_backend())


def set_up_cameras():
    """Set up camera matrices and positions with correct geometric transformation."""
    print("\n=== SETTING UP CAMERAS ===", flush=True)
    
    # Camera intrinsic matrices (same for both cameras)
    K = np.array([
        [1000, 0, 640],
        [0, 1000, 360], 
        [0, 0, 1]
    ], dtype=np.float64)
    
    # Camera world positions (where cameras are located in 3D space)
    cam1_center = np.array([0.0, 0.0, 2.5])  # Camera 1 at origin, raised in Z
    cam2_center = np.array([1.0, 0.0, 2.5])  # Camera 2 translated along X axis, raised in Z
    
    # Both cameras look in the -Z direction (standard computer vision convention)
    # Rotation matrices (identity = looking along -Z axis)
    R1 = np.eye(3, dtype=np.float64)
    R2 = np.eye(3, dtype=np.float64)
    
    # Translation vectors for projection matrices
    # t = -R * C where C is the camera center in world coordinates
    t1 = -np.dot(R1, cam1_center.reshape(3, 1))
    t2 = -np.dot(R2, cam2_center.reshape(3, 1))
    
    print(f"Camera 1 world position: {cam1_center}")
    print(f"Camera 2 world position: {cam2_center}")
    print(f"Camera 1 translation vector: {t1.flatten()}")
    print(f"Camera 2 translation vector: {t2.flatten()}")
    
    # Projection matrices P = K[R|t]
    P1 = np.dot(K, np.hstack((R1, t1)))
    P2 = np.dot(K, np.hstack((R2, t2)))
    
    print(f"Camera 1 projection matrix shape: {P1.shape}")
    print(f"Camera 2 projection matrix shape: {P2.shape}")
    
    return P1, P2, cam1_center, cam2_center


def plot_camera_fov_on_plane(ax, camera_pos, color, fov_angle_deg=60, plane_y=0.5, max_distance=4.0):
    """Plot camera field of view intersection with y=plane_y plane.
    
    Note: Since cameras look in -Z direction and plane is at y=0.5 (parallel to optical axis),
    the FOV intersection creates an elongated trapezoid, not a rectangle.
    A rectangle would only appear if the plane were perpendicular to the optical axis.
    """
    
    # Camera intrinsic parameters - assuming square pixels and symmetric FOV
    fov_horizontal = np.radians(fov_angle_deg)  # Horizontal FOV
    fov_vertical = np.radians(fov_angle_deg * 0.75)  # Vertical FOV (4:3 aspect ratio)
    
    # Distance from camera to the y=0.5 plane
    dy = abs(camera_pos[1] - plane_y)
    
    # For cameras looking in -Z direction, the FOV projection onto y=plane_y 
    # creates a trapezoid because the plane is parallel to the optical axis
    
    # Calculate actual geometric FOV boundaries on the y=0.5 plane
    z_near = camera_pos[2] - 0.2  # Start slightly in front of camera
    z_far = camera_pos[2] - max_distance  # End at max distance
    
    # At any Z distance, calculate the FOV width on the y=0.5 plane
    # The FOV cone extends in X and Y directions from the camera
    
    # For the y=0.5 plane (offset by dy from camera's y=0 position):
    # The vertical FOV component determines how the cone intersects this plane
    
    z_points = np.linspace(z_near, z_far, 20)
    
    # Calculate FOV boundaries at each Z depth
    x_left_boundary = []
    x_right_boundary = []
    
    for z in z_points:
        # Distance from camera to this point on the plane
        distance_to_point = np.sqrt((z - camera_pos[2])**2 + dy**2)
        
        # Half-width of FOV at this distance (horizontal FOV)
        half_width = distance_to_point * np.tan(fov_horizontal / 2)
        
        # However, we need to account for the geometry of the intersection
        # The effective width depends on the angle of intersection
        if dy > 0.001:
            # Calculate the actual intersection geometry
            # The plane cuts through the FOV cone at an angle
            effective_half_width = half_width
        else:
            effective_half_width = half_width
            
        x_left_boundary.append(camera_pos[0] - effective_half_width)
        x_right_boundary.append(camera_pos[0] + effective_half_width)
    
    # Convert to numpy arrays
    x_left = np.array(x_left_boundary)
    x_right = np.array(x_right_boundary)
    y_plane = np.full_like(z_points, plane_y)
    
    # Plot the FOV boundaries - this creates a trapezoid, not a rectangle
    # because the plane is parallel to the optical axis
    ax.plot(x_left, y_plane, z_points, 
            color=color, linewidth=3, alpha=0.8, linestyle='-', 
            label=f'{color.title()} FOV (Trapezoid)')
    
    ax.plot(x_right, y_plane, z_points, 
            color=color, linewidth=3, alpha=0.8, linestyle='-')
    
    # Connect near and far ends
    ax.plot([x_left[0], x_right[0]], [plane_y, plane_y], [z_points[0], z_points[0]], 
            color=color, linewidth=3, alpha=0.8, linestyle='-')
    ax.plot([x_left[-1], x_right[-1]], [plane_y, plane_y], [z_points[-1], z_points[-1]], 
            color=color, linewidth=3, alpha=0.8, linestyle='-')
    
    # Add a note about the geometry
    mid_x = camera_pos[0]
    mid_z = (z_points[0] + z_points[-1]) / 2
    ax.text(mid_x, plane_y + 0.1, mid_z, 
            f'{color.title()}\nTrapezoid\n(not rectangle)', 
            color=color, fontsize=8, ha='center', va='bottom')
    print(f"Added {color} camera FOV on y={plane_y} plane - creates trapezoid due to parallel intersection")

def plot_rectangular_fov_reference(ax, camera_pos, color, fov_angle_deg=60, plane_y=0.5, distance=2.0):
    """Show what the FOV would look like as a rectangle if camera looked perpendicular to plane."""
    
    # Calculate what a rectangular FOV would look like if the camera 
    # were positioned to look perpendicular to the y=0.5 plane
    
    fov_rad = np.radians(fov_angle_deg)
    half_fov = fov_rad / 2
    
    # If camera looked directly at the plane from distance 'distance'
    # Position camera at (camera_pos[0], plane_y - distance, camera_pos[2])
    virtual_cam_y = plane_y - distance
    
    # Rectangle dimensions on the plane
    half_width = distance * np.tan(half_fov)
    half_height = half_width * 0.75  # Assuming 4:3 aspect ratio
    
    # Rectangle corners on the y=0.5 plane
    x_min = camera_pos[0] - half_width
    x_max = camera_pos[0] + half_width
    z_min = camera_pos[2] - half_height
    z_max = camera_pos[2] + half_height
    
    # Draw the rectangular FOV outline
    rect_x = [x_min, x_max, x_max, x_min, x_min]
    rect_z = [z_min, z_min, z_max, z_max, z_min]
    rect_y = [plane_y] * 5
    
    ax.plot(rect_x, rect_y, rect_z, 
            color=color, linewidth=2, alpha=0.5, linestyle=':', 
            label=f'{color.title()} Rectangular FOV (reference)')
    
    # Add text annotation
    ax.text(camera_pos[0], plane_y + 0.2, camera_pos[2], 
            f'{color.title()}\nRectangle\n(if perpendicular)', 
            color=color, fontsize=8, ha='center', va='bottom', alpha=0.7)



def plot_camera(ax, camera_pos, color, label, boresight_length=3.0):
    """Plot a camera in 3D space with frustum and boresight line of sight."""
    print(f"Plotting camera: {label} at position {camera_pos}")
    
    # Plot camera position with a large marker
    ax.scatter(camera_pos[0], camera_pos[1], camera_pos[2], 
              color=color, marker='*', s=1000, label=label,
              edgecolor='black', linewidth=2.0)
    
    # Add a vertical stem line from ground to camera
    ax.plot([camera_pos[0], camera_pos[0]], 
            [camera_pos[1], camera_pos[1]], 
            [0, camera_pos[2]], 
            color=color, linewidth=4.0, alpha=0.7)
    
    # Add text label with coordinates
    ax.text(camera_pos[0], camera_pos[1], camera_pos[2] + 0.3,
            f"{label}\n({camera_pos[0]:.1f}, {camera_pos[1]:.1f}, {camera_pos[2]:.1f})",
            color=color, fontsize=10, weight='bold', ha='center')
    
    # Define boresight direction (looking forward and down at 45 degrees)
    elevation_angle = -45  # degrees
    azimuth_angle = 0  # degrees
    
    # Convert angles to direction vector
    elevation_rad = np.radians(elevation_angle)
    azimuth_rad = np.radians(azimuth_angle)
    
    boresight_dir = np.array([
        np.cos(elevation_rad) * np.sin(azimuth_rad),
        np.cos(elevation_rad) * np.cos(azimuth_rad),
        np.sin(elevation_rad)
    ])
    
    # Normalize the direction vector
    boresight_dir = boresight_dir / np.linalg.norm(boresight_dir)
    
    # Calculate boresight end point
    boresight_end = camera_pos + boresight_dir * boresight_length
    
    # Draw boresight line of sight
    ax.plot([camera_pos[0], boresight_end[0]],
            [camera_pos[1], boresight_end[1]],
            [camera_pos[2], boresight_end[2]],
            color=color, linewidth=2.0, linestyle='--',
            alpha=0.7)
    
    print(f"Camera {label} plotted successfully")


def generate_synthetic_tracks():
    """Generate synthetic 3D track and project to 2D camera views."""
    # Get camera matrices
    P1, P2, _, _ = set_up_cameras()
    
    print("\n=== GENERATING SYNTHETIC TRACKS ===", flush=True)
    
    # Define a 3D trajectory (moving object)
    # Move trajectory further from cameras for proper field of view
    # Cameras at Z=2.5, objects at Z=5.0→4.6 (further in front, visible range)
    original_3d = [
        np.array([0.2, 1.0, 2.7]),  # Further from cameras for proper FOV
        np.array([0.3, 1.0, 2.6]), 
        np.array([0.4, 1.0, 2.5]),
        np.array([0.5, 1.0, 2.4]),
        np.array([0.6, 1.0, 2.3])
    ]
    
    sensor1_track = []
    sensor2_track = []
    
    for i, point_3d in enumerate(original_3d):
        print(f"Processing point {i}: {point_3d}")
        
        # Convert to homogeneous coordinates
        point_3d_h = np.append(point_3d, 1.0)
        
        # Project to camera 1
        proj1 = np.dot(P1, point_3d_h)
        if abs(proj1[2]) > 1e-10:  # Check for valid depth
            pixel1 = proj1[:2] / proj1[2]
        else:
            print(f"Warning: Invalid depth for camera 1 at point {i}")
            pixel1 = np.array([np.inf, np.inf])
        
        # Project to camera 2  
        proj2 = np.dot(P2, point_3d_h)
        if abs(proj2[2]) > 1e-10:  # Check for valid depth
            pixel2 = proj2[:2] / proj2[2]
        else:
            print(f"Warning: Invalid depth for camera 2 at point {i}")
            pixel2 = np.array([np.inf, np.inf])
        
        sensor1_track.append(pixel1)
        sensor2_track.append(pixel2)
        
        print(f"  Camera 1 pixel: {pixel1}")
        print(f"  Camera 2 pixel: {pixel2}")
    
    # Convert to numpy arrays
    sensor1_track = np.array(sensor1_track)
    sensor2_track = np.array(sensor2_track)
    original_3d = np.array(original_3d)
    
    print(f"\nGenerated tracks:")
    print(f"Sensor 1 track shape: {sensor1_track.shape}")
    print(f"Sensor 2 track shape: {sensor2_track.shape}")
    print(f"Original 3D shape: {original_3d.shape}")
    
    return sensor1_track, sensor2_track, original_3d


def triangulate_tracks(sensor1_track, sensor2_track, P1, P2):
    """Triangulate 3D points from 2D tracks from two cameras."""
    points_3d = []
    
    for pt1, pt2 in zip(sensor1_track, sensor2_track):
        # Skip invalid points
        if not (np.isfinite(pt1[0]) and np.isfinite(pt1[1]) and 
                np.isfinite(pt2[0]) and np.isfinite(pt2[1])):
            print(f"Skipping invalid points: {pt1}, {pt2}")
            points_3d.append(np.array([np.nan, np.nan, np.nan]))
            continue
            
        # Reshape points for OpenCV triangulation
        p1 = np.array(pt1, dtype=np.float32).reshape(2, 1)
        p2 = np.array(pt2, dtype=np.float32).reshape(2, 1)
        
        # Triangulate the point
        point_homog = cv2.triangulatePoints(P1, P2, p1, p2)
        
        # Convert from homogeneous coordinates
        if abs(point_homog[3]) > 1e-10:
            point_3d = (point_homog[:3] / point_homog[3]).flatten()
        else:
            point_3d = np.array([np.nan, np.nan, np.nan])
            
        points_3d.append(point_3d)
    
    return np.array(points_3d)


def plot_2d_camera_views(sensor1_track, sensor2_track, original_3d):
    """Plot separate 2D views showing the trajectory as seen by each camera."""
    print("\n=== CREATING 2D CAMERA VIEWS ===", flush=True)
    
    # Create figure with two subplots side by side
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
    
    # Camera 1 view (left plot)
    ax1.set_title('Camera 1 View\n(Object Trajectory in Image Plane)', fontsize=14, weight='bold')
    ax1.set_xlabel('X (pixels)', fontsize=12)
    ax1.set_ylabel('Y (pixels)', fontsize=12)
    
    # Plot the trajectory points
    for i, (pixel, point_3d) in enumerate(zip(sensor1_track, original_3d)):
        ax1.scatter(pixel[0], pixel[1], s=200, c='red', alpha=0.7, 
                   edgecolor='black', linewidth=2, zorder=5)
        ax1.annotate(f'P{i+1}\n3D:({point_3d[0]:.1f},{point_3d[1]:.1f},{point_3d[2]:.1f})', 
                    (pixel[0], pixel[1]), xytext=(10, 10), textcoords='offset points',
                    fontsize=10, ha='left', va='bottom',
                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))
    
    # Connect the points to show trajectory
    ax1.plot([p[0] for p in sensor1_track], [p[1] for p in sensor1_track], 
             'r-', linewidth=3, alpha=0.6, label='Trajectory')
    
    # Add directional arrows
    for i in range(len(sensor1_track)-1):
        dx = sensor1_track[i+1][0] - sensor1_track[i][0]
        dy = sensor1_track[i+1][1] - sensor1_track[i][1]
        ax1.arrow(sensor1_track[i][0], sensor1_track[i][1], 
                 dx*0.7, dy*0.7, head_width=15, head_length=10, 
                 fc='darkred', ec='darkred', alpha=0.8)
    
    # Set camera 1 image bounds (assuming 1280x720 image)
    ax1.set_xlim(0, 1280)
    ax1.set_ylim(720, 0)  # Invert Y axis (image coordinates)
    ax1.grid(True, alpha=0.3)
    ax1.legend(loc='upper right')
    
    # Camera 2 view (right plot)
    ax2.set_title('Camera 2 View\n(Object Trajectory in Image Plane)', fontsize=14, weight='bold')
    ax2.set_xlabel('X (pixels)', fontsize=12)
    ax2.set_ylabel('Y (pixels)', fontsize=12)
    
    # Plot the trajectory points
    for i, (pixel, point_3d) in enumerate(zip(sensor2_track, original_3d)):
        ax2.scatter(pixel[0], pixel[1], s=200, c='blue', alpha=0.7, 
                   edgecolor='black', linewidth=2, zorder=5)
        ax2.annotate(f'P{i+1}\n3D:({point_3d[0]:.1f},{point_3d[1]:.1f},{point_3d[2]:.1f})', 
                    (pixel[0], pixel[1]), xytext=(10, 10), textcoords='offset points',
                    fontsize=10, ha='left', va='bottom',
                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))
    
    # Connect the points to show trajectory
    ax2.plot([p[0] for p in sensor2_track], [p[1] for p in sensor2_track], 
             'b-', linewidth=3, alpha=0.6, label='Trajectory')
    
    # Add directional arrows
    for i in range(len(sensor2_track)-1):
        dx = sensor2_track[i+1][0] - sensor2_track[i][0]
        dy = sensor2_track[i+1][1] - sensor2_track[i][1]
        ax2.arrow(sensor2_track[i][0], sensor2_track[i][1], 
                 dx*0.7, dy*0.7, head_width=15, head_length=10, 
                 fc='darkblue', ec='darkblue', alpha=0.8)
    
    # Set camera 2 image bounds
    ax2.set_xlim(0, 1280)
    ax2.set_ylim(720, 0)  # Invert Y axis (image coordinates)
    ax2.grid(True, alpha=0.3)
    ax2.legend(loc='upper right')
    
    # Adjust layout
    plt.tight_layout()
    plt.show()
    
    # Print trajectory information
    print("\nCamera 1 Trajectory (pixels):")
    for i, (pixel, point_3d) in enumerate(zip(sensor1_track, original_3d)):
        print(f"  Point {i+1}: ({pixel[0]:.1f}, {pixel[1]:.1f}) <- 3D: ({point_3d[0]:.1f}, {point_3d[1]:.1f}, {point_3d[2]:.1f})")
    
    print("\nCamera 2 Trajectory (pixels):")
    for i, (pixel, point_3d) in enumerate(zip(sensor2_track, original_3d)):
        print(f"  Point {i+1}: ({pixel[0]:.1f}, {pixel[1]:.1f}) <- 3D: ({point_3d[0]:.1f}, {point_3d[1]:.1f}, {point_3d[2]:.1f})")
    
    print("\n2D camera views complete!")



# Test all functions
print("Testing camera setup...")
P1, P2, cam1_pos, cam2_pos = set_up_cameras()

print("\nTesting track generation...")
sensor1_track, sensor2_track, original_3d = generate_synthetic_tracks()

print("\nTesting triangulation...")
reconstructed_3d = triangulate_tracks(sensor1_track, sensor2_track, P1, P2)

print("\n=== ALL FUNCTIONS READY ===")
print("You can now run the visualization cells!")

# Verify the geometric correction worked
print("\n=== GEOMETRIC VERIFICATION ===")
print("3D Trajectory X coordinates:", [pt[0] for pt in original_3d])
print("Camera 1 pixel X coordinates:", [pt[0] for pt in sensor1_track])
print("Camera 2 pixel X coordinates:", [pt[0] for pt in sensor2_track])
print("\nExpected behavior:")
print("- Camera 1 (at X=0): Object moves RIGHT → pixels should INCREASE")
print("- Camera 2 (at X=1): Object moves LEFT relative to camera → pixels should DECREASE")


# Generate and display 2D camera views
plot_2d_camera_views(sensor1_track, sensor2_track, original_3d)



def plot_individual_camera_views(sensor1_track, sensor2_track, original_3d):
    """Plot individual detailed 2D views for each camera."""
    print("\n=== CREATING INDIVIDUAL CAMERA VIEWS ===", flush=True)
    
    # Camera 1 individual plot
    plt.figure(figsize=(12, 9))
    plt.title('Camera 1 - Object Trajectory in Image Plane', fontsize=16, weight='bold', pad=20)
    plt.xlabel('X (pixels)', fontsize=14)
    plt.ylabel('Y (pixels)', fontsize=14)
    
    # Plot trajectory points with enhanced visualization
    colors = ['darkred', 'red', 'orange', 'yellow', 'lightcoral']
    for i, (pixel, point_3d) in enumerate(zip(sensor1_track, original_3d)):
        plt.scatter(pixel[0], pixel[1], s=300, c=colors[i], alpha=0.8, 
                   edgecolor='black', linewidth=3, zorder=5, label=f'Point {i+1}')
        
        # Enhanced annotation with 3D coordinates
        plt.annotate(f'Point {i+1}\nPixel: ({pixel[0]:.0f}, {pixel[1]:.0f})\n3D: ({point_3d[0]:.1f}, {point_3d[1]:.1f}, {point_3d[2]:.1f})', 
                    (pixel[0], pixel[1]), xytext=(20, 20), textcoords='offset points',
                    fontsize=11, ha='left', va='bottom', weight='bold',
                    bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9, edgecolor='red'))
    
    # Draw trajectory line
    plt.plot([p[0] for p in sensor1_track], [p[1] for p in sensor1_track], 
             'r-', linewidth=4, alpha=0.7, label='Trajectory Path')
    
    # Add movement direction arrows
    for i in range(len(sensor1_track)-1):
        dx = sensor1_track[i+1][0] - sensor1_track[i][0]
        dy = sensor1_track[i+1][1] - sensor1_track[i][1]
        plt.arrow(sensor1_track[i][0], sensor1_track[i][1], 
                 dx*0.8, dy*0.8, head_width=25, head_length=20, 
                 fc='darkred', ec='darkred', alpha=0.9, linewidth=2)
    
    # Set image bounds and styling
    plt.xlim(0, 1280)
    plt.ylim(720, 0)  # Invert Y axis
    plt.grid(True, alpha=0.4, linestyle='--')
    plt.legend(loc='upper right', fontsize=12)
    
    # Add camera info
    plt.text(50, 50, 'Camera 1 Position: (0.0, 0.0, 2.5)', 
             fontsize=12, bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    
    plt.tight_layout()
    plt.show()
    
    # Camera 2 individual plot
    plt.figure(figsize=(12, 9))
    plt.title('Camera 2 - Object Trajectory in Image Plane', fontsize=16, weight='bold', pad=20)
    plt.xlabel('X (pixels)', fontsize=14)
    plt.ylabel('Y (pixels)', fontsize=14)
    
    # Plot trajectory points with enhanced visualization
    colors = ['darkblue', 'blue', 'cyan', 'lightblue', 'steelblue']
    for i, (pixel, point_3d) in enumerate(zip(sensor2_track, original_3d)):
        plt.scatter(pixel[0], pixel[1], s=300, c=colors[i], alpha=0.8, 
                   edgecolor='black', linewidth=3, zorder=5, label=f'Point {i+1}')
        
        # Enhanced annotation with 3D coordinates
        plt.annotate(f'Point {i+1}\nPixel: ({pixel[0]:.0f}, {pixel[1]:.0f})\n3D: ({point_3d[0]:.1f}, {point_3d[1]:.1f}, {point_3d[2]:.1f})', 
                    (pixel[0], pixel[1]), xytext=(20, 20), textcoords='offset points',
                    fontsize=11, ha='left', va='bottom', weight='bold',
                    bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9, edgecolor='blue'))
    
    # Draw trajectory line
    plt.plot([p[0] for p in sensor2_track], [p[1] for p in sensor2_track], 
             'b-', linewidth=4, alpha=0.7, label='Trajectory Path')
    
    # Add movement direction arrows
    for i in range(len(sensor2_track)-1):
        dx = sensor2_track[i+1][0] - sensor2_track[i][0]
        dy = sensor2_track[i+1][1] - sensor2_track[i][1]
        plt.arrow(sensor2_track[i][0], sensor2_track[i][1], 
                 dx*0.8, dy*0.8, head_width=25, head_length=20, 
                 fc='darkblue', ec='darkblue', alpha=0.9, linewidth=2)
    
    # Set image bounds and styling
    plt.xlim(0, 1280)
    plt.ylim(720, 0)  # Invert Y axis
    plt.grid(True, alpha=0.4, linestyle='--')
    plt.legend(loc='upper right', fontsize=12)
    
    # Add camera info
    plt.text(50, 50, 'Camera 2 Position: (1.0, 0.0, 2.5)', 
             fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))
    
    plt.tight_layout()
    plt.show()
    
    print("\nIndividual camera views complete!")



# Create a large figure for better interaction
plt.figure(figsize=(16, 12))
ax = plt.axes(projection='3d')

# Set consistent view limits with more space
ax.set_xlim(-1.0, 2.0)
ax.set_ylim(-1.0, 1.5)
ax.set_zlim(-0.5, 4.0)

# Enable grid for better spatial reference
ax.grid(True, linestyle='--', alpha=0.6)

# Plot ground plane
xx, yy = np.meshgrid(np.linspace(-1.0, 2.0, 20), np.linspace(-1.0, 1.5, 20))
zz = np.zeros_like(xx)
ax.plot_surface(xx, yy, zz, alpha=0.2, color='gray')

# Plot trajectory constraint plane (y = 1.0)
xx_plane, zz_plane = np.meshgrid(np.linspace(-1.0, 2.0, 20), np.linspace(-0.5, 4.0, 20))
yy_plane = np.full_like(xx_plane, 1.0)
ax.plot_surface(xx_plane, yy_plane, zz_plane, alpha=0.1, color='yellow', label='Trajectory Plane (y=1.0)')

# Get camera positions
_, _, cam1_pos, cam2_pos = set_up_cameras()

# Plot cameras with increased boresight length for clarity
plot_camera(ax, cam1_pos, 'red', 'Camera 1', boresight_length=4.0)
plot_camera(ax, cam2_pos, 'blue', 'Camera 2', boresight_length=4.0)

# Calculate required FOV to cover trajectory
# Trajectory spans from x=0.2 to x=0.6 and z=2.6 to z=3.0 on y=1.0 plane
# Camera 1 at [0, 0, 2.5], Camera 2 at [1, 0, 2.5]

# For Camera 1: furthest point is [0.6, 1.0, 2.6]
# Distance from cam1 to this point
import math
cam1_to_far_x = 0.6 - 0.0  # 0.6
cam1_to_far_y = 1.0 - 0.0  # 1.0  
cam1_to_far_z = 2.6 - 2.5  # 0.1
cam1_distance = math.sqrt(cam1_to_far_x**2 + cam1_to_far_y**2 + cam1_to_far_z**2)

# Required FOV angle to see from center to edge of trajectory
required_angle_cam1 = math.degrees(2 * math.atan(cam1_to_far_x / math.sqrt(cam1_to_far_y**2 + cam1_to_far_z**2)))

# For Camera 2: furthest point is [0.2, 1.0, 2.6] 
cam2_to_far_x = abs(0.2 - 1.0)  # 0.8
cam2_to_far_y = 1.0 - 0.0  # 1.0
cam2_to_far_z = 2.6 - 2.5  # 0.1
required_angle_cam2 = math.degrees(2 * math.atan(cam2_to_far_x / math.sqrt(cam2_to_far_y**2 + cam2_to_far_z**2)))

# Use larger angle with some margin
fov_angle = max(required_angle_cam1, required_angle_cam2, 70)  # At least 70 degrees
print(f"Required FOV: Cam1={required_angle_cam1:.1f}°, Cam2={required_angle_cam2:.1f}°")
print(f"Using FOV angle: {fov_angle:.1f}°")

# Plot simplified camera fields of view on the y=1.0 plane (no trapezoid outlines)
plot_rectangular_fov_reference(ax, cam1_pos, 'red', fov_angle_deg=fov_angle, plane_y=1.0, distance=1.0)
plot_rectangular_fov_reference(ax, cam2_pos, 'blue', fov_angle_deg=fov_angle, plane_y=1.0, distance=1.0)

# Plot all 3D points with increased size and better visibility
ax.scatter(
    [p[0] for p in original_3d],
    [p[1] for p in original_3d],
    [p[2] for p in original_3d],
    color='green', label='Original Track', s=200, alpha=0.7
)

# Only plot valid reconstructed points
valid_mask = ~np.isnan(reconstructed_3d[:, 0])
if np.any(valid_mask):
    ax.scatter(
        reconstructed_3d[valid_mask, 0],
        reconstructed_3d[valid_mask, 1],
        reconstructed_3d[valid_mask, 2],
        color='orange', label='Reconstructed Track', s=200, alpha=0.7
    )

# Add trajectory lines with increased thickness
ax.plot(
    [p[0] for p in original_3d],
    [p[1] for p in original_3d],
    [p[2] for p in original_3d],
    'g-', linewidth=3, alpha=0.5
)

if np.any(valid_mask):
    ax.plot(
        reconstructed_3d[valid_mask, 0],
        reconstructed_3d[valid_mask, 1],
        reconstructed_3d[valid_mask, 2],
        color='orange', linewidth=3, alpha=0.5
    )

# Add frame numbers next to points
for i, orig in enumerate(original_3d):
    ax.text(orig[0], orig[1], orig[2], f' {i}', color='green', fontsize=12)

if np.any(valid_mask):
    for i, recon in enumerate(reconstructed_3d[valid_mask]):
        ax.text(recon[0], recon[1], recon[2], f' {i}', color='orange', fontsize=12)

# Set labels with increased size
ax.set_xlabel('X (meters)', fontsize=14, labelpad=10)
ax.set_ylabel('Y (meters)', fontsize=14, labelpad=10)
ax.set_zlabel('Z (meters)', fontsize=14, labelpad=10)

# Increase tick label size
ax.tick_params(axis='both', which='major', labelsize=12)

# Add title
plt.title('3D Track Reconstruction\nStatic Plot (No JavaScript Dependencies)', 
          fontsize=16, pad=20)

# Move legend outside the plot
ax.legend(bbox_to_anchor=(1.15, 1), loc='upper right', fontsize=12)

# Set consistent view angle
ax.view_init(elev=20, azim=-40)

plt.tight_layout()
plt.show()

print("\n3D visualization complete!")


# Generate individual detailed camera views
plot_individual_camera_views(sensor1_track, sensor2_track, original_3d)

